{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center>\n",
    "<h1> AWS SageMaker </h1>\n",
    "    <h2>MLOps using AWS SageMaker </h2>\n",
    "    <h3>March 23, 2023</h3>\n",
    "<hr>\n",
    "<h1>Exploratory Data Analysis of Wind Turbine Dataset</h1>\n",
    "<hr>\n",
    " </center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](pic/turbines_winji.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "We start our journey into MLOps with AWS SageMaker by inspecting the dataset with regards to:\n",
    "\n",
    "1. Data composition, with regards to structure and quality\n",
    "2. Associations between the different attributes\n",
    "\n",
    "LetÂ´s get started by installing some of the non-default libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install seaborn==0.11.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to the s3 bucket"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\"> ðŸŽ¯ <strong> Connect to S3 bucket with wind turbine data </strong>\n",
    "    \n",
    "Connect to the bucket where to obfuscated wind turbine data from [winji](https://www.win-ji.com/) is located.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data paths\n",
    "BUCKET = 's3://sagemaker-d-one-winji-data'\n",
    "RAW_DATA_FOLDER = 'data'\n",
    "RAW_DATA_FILE = 'wind_turbines.csv'\n",
    "RAW_DATA_PATH = os.path.join(BUCKET, RAW_DATA_FOLDER, RAW_DATA_FILE)\n",
    "\n",
    "\n",
    "df = pd.read_csv(RAW_DATA_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the data composition\n",
    "we do this by examining:\n",
    "* quick look into the data \n",
    "* dimension, how many rows and columns?\n",
    "* fraction of missing data\n",
    "* datatypes of each attribute"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sneak peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have an idea how the data looks like let us display one record of the dataset. The data will become clearer when looking at it in more detail later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we a total of 16 attributes and 52383 rows. Lets next answer how much of the data is missing and what each attribute is composed of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fraction of missing data and datatype per attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame([df.isna().mean(), df.dtypes, df.nunique()])\n",
    "   .T\n",
    "   .rename({0:'fraction of na',\n",
    "            1:'datatype',\n",
    "            2:'n_unique_entries'}, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['measured_at'].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this little breakdown we learn that the data consists of 16 attributes of which 15 are numerical and 1 is a timestamp. There are a total of 52383 entries in this dataset with no missing data across the feature attributes. The columns with missing data are the categories we want to predict. The `subtraction` attribute shows the presence of an error and the `categories_sk` attribute descibes the type of error in more depth. \n",
    "Since the aim of this exersice is to showcase this dataset and ultimatly a SageMaker workflow we continue with the `subtraction` attribute and drop the `categories_sk` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"categories_sk\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is time series data we wanted to see if the data was homogeniously collected over the time span. Thus we first parsed the `measured_at` column into a machine readable datetime and plotted the number of rows over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['measured_at'] = pd.to_datetime(df['measured_at'])\n",
    "\n",
    "plt.hist(df.measured_at.values)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The data is evenly spread over the sampled time horizon from 2020-01 to 2020-07\n",
    "\n",
    "Now, lets have a look if, on a high level, we can connect the turbine features with an error type."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Associations between the different data attributes\n",
    "\n",
    "Ultimatly we wanted to see the highdimensional structure. We did this by:\n",
    "   1. z-scoring features excluding the time dimension, turbine number and error code\n",
    "   2. Collapsing the high dimensional data into 2 dimensions using umap\n",
    "   3. Highlighting excluded attributes on the 2d representation of the data\n",
    "\n",
    "Feel free to think about what the output of the umap means for our machine learning model. Below the figures you'll find a write up with our interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Â Preparing the data for the plots\n",
    "# Split data into labels and features\n",
    "columns = df.columns.values\n",
    "y_cols = df.columns.str.contains('wt_sk|subtraction|measured_at')\n",
    "\n",
    "y = df[list(columns[y_cols])]\n",
    "X = df[list(columns[~y_cols])]\n",
    "\n",
    "# 1. Scale features\n",
    "scaler = StandardScaler()\n",
    "X_ = scaler.fit_transform(X)\n",
    "\n",
    "# 2. Get embeddings\n",
    "embedding = umap.UMAP().fit_transform(X_)\n",
    "\n",
    "# Curate dataframe for plotting\n",
    "X_to_plot = pd.DataFrame(embedding, columns=['UMAP1','UMAP2'])\n",
    "X_to_plot['error_type'] = y['subtraction'].to_list()\n",
    "\n",
    "# Flag na values with 2 in error type\n",
    "X_to_plot['error_type'][X_to_plot.error_type.isna()] = 2\n",
    "X_to_plot['is_error'] = X_to_plot.error_type != 2\n",
    "\n",
    "X_to_plot = X_to_plot.sort_values(by='is_error', ascending=True)\n",
    "\n",
    "X_to_plot['wt_sk'] = y['wt_sk'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1\n",
    "p = sns.jointplot(data=X_to_plot,\n",
    "                x='UMAP1',\n",
    "                y='UMAP2',\n",
    "                space= 0,\n",
    "                alpha=0.5,\n",
    "                kind='hex'\n",
    ")\n",
    "plt.suptitle(\"Figure 1: Density of feature distribution\", y = 1)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the dimensionality reduction technique we deployed above (we used UMAP) we make the following observations:\n",
    "\n",
    "1. In the first plot we see that the there is not a single point where most of the data is located but that it is spread across the different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2\n",
    "sns.jointplot(data=X_to_plot,\n",
    "                x='UMAP1',\n",
    "                y='UMAP2',\n",
    "                space= 0,\n",
    "                alpha=0.5,\n",
    "                hue='wt_sk',\n",
    ")\n",
    "plt.suptitle(\"Figure 2: Features colored by turbine number\", y = 1)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In the second plot the color indicates the different turbines. Since the colors are equally spread across the dimensions we can assume that there is no strong bias in the data towards one turbine in specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3\n",
    "sns.jointplot(data=X_to_plot,\n",
    "                x='UMAP1',\n",
    "                y='UMAP2',\n",
    "                space= 0,\n",
    "                alpha=0.5,\n",
    "                hue='is_error',\n",
    ")\n",
    "plt.suptitle(\"Figure 3: Features colored by error\", y = 1)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In the third plot the color indicates wether there is an error or not. We find, as expected, that the errors are mostly located on separated islands with some exceptions where the blue and orange points, corresponding to faulty and working turbines, are mixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4\n",
    "sns.jointplot(data=X_to_plot,\n",
    "                x='UMAP1',\n",
    "                y='UMAP2',\n",
    "                space= 0,\n",
    "                alpha=0.5,\n",
    "                hue='error_type',\n",
    ")\n",
    "plt.suptitle(\"Figure 4: Features colored by error type\", y = 1)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Further looking into the error type we find that the different error types are distinct from each other.\n",
    "\n",
    "We conclude that we can proceed with training a model on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\"> ðŸŽ‰ <strong> Data looks great! Lets try to model it </strong>\n",
    "\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
